# Configurable parameters and default values for splunk-kubernetes-logging.
# This is a YAML-formatted file.
# Declared variables will be passed into templates.

# logLevel is to set log level of the Splunk log collector. Avaiable values are:
# * trace
# * debug
# * info (default)
# * warn
# * error
logLevel:

# Namespace to deploy agent in (Optional: Will default to release namespace)
namespace:

# This is can be used to exclude verbose logs including various system and Helm/Tiller related logs.
fluentd:
  # path of logfiles, default /var/log/containers/*.log
  path: /var/log/containers/*.log
  # paths of logfiles to exclude. object type is array as per fluentd specification:
  # https://docs.fluentd.org/input/tail#exclude_path
  exclude_path:
  #  - /var/log/containers/kube-svc-redirect*.log
  #  - /var/log/containers/tiller*.log
  #  - /var/log/containers/*_kube-system_*.log (to exclude `kube-system` namespace)

# Configurations for container logs
containers:
  # Path to root directory of container logs
  path: /var/log
  # Final volume destination of container log symlinks
  pathDest: /var/lib/docker/containers
  # Log format type, "json" or "cri"
  logFormatType: json
  # Specify the logFormat for "cri" logFormatType - provide time format
  # For example "%Y-%m-%dT%H:%M:%S.%N%:z" for openshift, "%Y-%m-%dT%H:%M:%S.%NZ" for IBM IKS
  # Default for "cri": "%Y-%m-%dT%H:%M:%S.%N%:z"
  # For "json", the log format cannot be changed: "%Y-%m-%dT%H:%M:%S.%NZ"
  logFormat:
  # Specify the interval of refreshing the list of watch file.
  refreshInterval:
  # Boolean to whether to remove blank events or not
  removeBlankEvents: true
  # Boolean if true, uses local time. UTC. Otherwise, UTC is used
  localTime: false
  # Boolean to whether enables the additional inotify-based watcher.
  # Setting this parameter to false will disable the inotify events and use only timer watcher for file tailing.
  enableStatWatcher: true

# Directory where to read journald logs (docker daemon logs, kubelet logs, and any other specified service logs).
# Journald will use `/var/log/journal` automagically if the directory exists. For example on OpenShift this 
# directory is used for persistent log storage.
journalLogPath: /run/log/journal

# Enriches pod log record with kubernetes data
k8sMetadata:
  # Pod labels to collect
  podLabels:
    - app
    - k8s-app
    - release
  watch: true
  cache_ttl: 3600
  # Boolean to propagate listed labels also from namespace if none is found in pod
  propagate_namespace_labels: false

sourcetypePrefix: "kube"

# Local splunk configurations
splunk:
  # Configurations for HEC (HTTP Event Collector)
  hec:
    # host is required and should be provided by user
    host:
    # port to HEC, optional, default 8088
    port:
    # token is required and should be provided by user
    token:
    # protocol has two options: "http" and "https", default is "https"
    protocol:
    # Instead of providing host, port, and protocol, you can provide full url for splunk. For example: https://mydomain.com:8088/apps/splunk
    fullUrl:
    # indexName tells which index to use, this is optional. If it's not present, will use the "main".
    indexName:
    # insecureSSL is a boolean, it indicates should it allow insecure SSL connection (when protocol is "https"). Default is false.
    insecureSSL:
    # The PEM-format CA certificate for this client.
    # NOTE: The content of the certificate itself should be used here, not the file path.
    #       The certificate will be stored as a secret in kubernetes.
    #       Make sure you are providing the certificate as multiline string (use `|-`)
    # Example:
    # caFile: |-
    #   -----BEGIN CERTIFICATE-----
    #   ...
    #   -----END CERTIFICATE-----
    clientCert:
    # The private key for this client.
    # NOTE: The content of the key itself should be used here, not the file path.
    #       The key will be stored as a secret in kubernetes.
    clientKey:
    # The PEM-format CA certificate file.
    # NOTE: The content of the file itself should be used here, not the file path.
    #       The file will be stored as a secret in kubernetes.
    caFile:
    # Indicates if 4xx errors should consume chunk. If set to true, plugin will not retry sending chunk to splunk when 4xx error occurs. (Default: true)
    consume_chunk_on_4xx_errors:
    # gzip_compression - boolean. If true, data is compressed before transfer to splunk. (Default: false)
    gzip_compression:
  # Configurations for Ingest API
  ingest_api:
    # serviceClientIdentifier is a string, the client identifier is used to make requests to the ingest API with authorization.
    serviceClientIdentifier:
    # serviceClientSecretKey is a string, the client identifier is used to make requests to the ingest API with authorization.
    serviceClientSecretKey:
    # tokenEndpoint is a string, it indicates which endpoint should be used to get the authorization token used to make requests to the ingest API.
    tokenEndpoint:
    # ingestAuthHost is a string, it indicates which url/hostname should be used to make token auth requests to the ingest API.
    ingestAuthHost:
    # ingestAPIHost is a string, it indicates which url/hostname should be used to make requests to the ingest API.
    ingestAPIHost:
    # tenant is a string, it indicates which tenant should be used to make requests to the ingest API.
    tenant:
    # eventsEndpoint is a string, it indicates which endpoint should be used to make requests to the ingest API.
    eventsEndpoint:
    # debugIngestAPI is a boolean, it indicates whether user wants to debug requests and responses to ingest API. Default is false.
    debugIngestAPI:

rbac:
  # Specifies whether RBAC resources should be created.
  # This should be set to `false` if either:
  # a) RBAC is not enabled in the cluster, or
  # b) you want to create RBAC resources by yourself.
  create: true
  # If you are on OpenShift and you want to run the a privileged pod
  # you need to have a ClusterRoleBinding for the system:openshift:scc:privileged
  # ClusterRole. Set to `true` to create the ClusterRoleBinding resource
  # for the ServiceAccount.
  openshiftPrivilegedSccBinding: false

serviceAccount:
  # Specifies whether a ServiceAccount should be created
  create: true
  # The name of the ServiceAccount to use.
  # If not set and create is true, a name is generated using the fullname template
  name:

podSecurityPolicy:
  # Specifies whether Pod Security Policy resources should be created.
  # This should be set to `false` if either:
  # a) Pod Security Policies is not enabled in the cluster, or
  # b) you want to create Pod Security Policy resources by yourself.
  create: false
  # Specifies whether AppArmor profile should be applied.
  # if set to true, this will add two annotations to PodSecurityPolicy:
  # apparmor.security.beta.kubernetes.io/allowedProfileNames: 'runtime/default'
  # apparmor.security.beta.kubernetes.io/defaultProfileName:  'runtime/default'
  # set to false if AppArmor is not available
  apparmor_security: true

# Create or use existing secret if name is empty default name is used
secret:
  create: true
  name:

# Set to true, to change the encoding of all strings to utf-8.
#
# By default fluentd uses ASCII-8BIT encoding. If you have 2-byte chars in your logs
# you need to set the encoding to UTF-8 instead.
#
charEncodingUtf8: false

# `logs` defines the source of logs, multiline support, and their sourcetypes.
#
# The scheme to define a log is:
#
# ```
# <name>:
#   from:
#     <source>
#   timestampExtraction:
#     regexp: "<regexp_to_extract_timestamp_from_log>"
#     format: "<format_of_the_timestamp>"
#   multiline:
#     firstline: "<regexp_to_detect_firstline_of_multiline>"
#     flushInterval: 5
#     separator: ""
#   sourcetype: "<sourcetype_of_logs>"
#   limitRecentlyModified: <time_duration>
# ```
#
# = <source> =
# It supports 3 kinds of sources: journald, file, and container.
# For `journald` logs, `unit` is required for filtering using _SYSTEMD_UNIT, example:
# ```
# docker:
#   from:
#     journald:
#       unit: docker.service
# ```
#
# For `file` logs, `path` is required for specifying where is the log files. Log files are expected in `/var/log`, example:
# ```
# docker:
#   from:
#     file:
#       path: /var/log/docker.log
# ```
#
# For `container` logs, pod name is required. You can also provide the container name, if it's not provided, the name of this source will be used as the container name:
# ```
# kube-apiserver:
#   from:
#     pod: kube-apiserver
#
# etcd:
#   from:
#     pod: etcd-server
#     container: etcd-container
# ```
#
# = timestamp =
# `timestampExtraction` defines how to extract timestamp from logs. This *only* works for `file` source.
# To use `timestampExtraction` you need to define both:
# - `regexp`: the Regular Expression used to find the timestamp from a log entry.
#             The timestamp part must be in a `time` named group. E.g.
#             (?<time>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})
# - `format`: a format string defintes how to parse the timestamp, e.g. "%Y-%m-%d %H:%M:%S".
#             More details can be find: http://ruby-doc.org/stdlib-2.5.0/libdoc/time/rdoc/Time.html#method-c-strptime
#
# = multiline =
# `multiline` options provide basic multiline support. Two options:
# - `firstline`: a Regular Expression used to detect the first line of a multiline log.
# - `endline`: a Regular Expression used to detect the end line of a multiline log.
# - `flushInterval`: The number of seconds after which the last received event log will be flushed, default value: 5.
# - `separator`: The separator of lines.  This *only* works for `container` source.
#
# = sourcetype =
# sourcetype of each kind of log can be defined using the `sourcetype` field.
# If `sourcetype` is not defined, `name` will be used.
#
# = limitRecentlyModified =
# Limits the watching files that the modification time is within the specified time period when using * in path.
# - <INTEGER>s: seconds
# - <INTEGER>m: minutes
# - <INTEGER>h: hours
# - <INTEGER>d: days
# - Otherwise, the field is parsed as float, and that float is the number of seconds
# Example, specifying `limitRecentlyModified: 24h` will parse only files with modification time within 24 hours.
# Files modified earlier will be skipped.
# If `limitRecentlyModified` is not defined, this option will be disabled.
#
# ---
# Here we have some default timestampExtraction and multiline settings for kubernetes components.
# So, usually you just need to redefine the source of those components if necessary.
logs:
  docker:
    from:
      journald:
        unit: docker.service
    sourcetype: kube:docker
  kubelet: &glog
    from:
      journald:
        unit: kubelet.service
    multiline:
      firstline: /^\w[0-1]\d[0-3]\d/
    sourcetype: kube:kubelet
  etcd:
    from:
      pod: etcd-server
      container: etcd-container
  etcd-minikube:
    from:
      pod: etcd-minikube
      container: etcd
  etcd-events:
    from:
      pod: etcd-server-events
      container: etcd-container
  kube-apiserver:
    <<: *glog
    from:
      pod: kube-apiserver
    sourcetype: kube:kube-apiserver
  kube-scheduler:
    <<: *glog
    from:
      pod: kube-scheduler
    sourcetype: kube:kube-scheduler
  kube-controller-manager:
    <<: *glog
    from:
      pod: kube-controller-manager
    sourcetype: kube:kube-controller-manager
  kube-proxy:
    <<: *glog
    from:
      pod: kube-proxy
    sourcetype: kube:kube-proxy
  kubedns:
    <<: *glog
    from:
      pod: kube-dns
    sourcetype: kube:kubedns
  dnsmasq:
    <<: *glog
    from:
      pod: kube-dns
    sourcetype: kube:dnsmasq
  dns-sidecar:
    <<: *glog
    from:
      pod: kube-dns
      container: sidecar
    sourcetype: kube:kubedns-sidecar
  dns-controller:
    <<: *glog
    from:
      pod: dns-controller
    sourcetype: kube:dns-controller
  kube-dns-autoscaler:
    <<: *glog
    from:
      pod: kube-dns-autoscaler
      container: autoscaler
    sourcetype: kube:kube-dns-autoscaler
  kube-audit:
    from:
      file:
        path: /var/log/kube-apiserver-audit.log
    timestampExtraction:
      format: "%Y-%m-%dT%H:%M:%SZ"
    sourcetype: kube:apiserver-audit

# Defines which version of image to use, and how it should be pulled.
image:
  # The domain of the registry to pull the image from
  registry: docker.io
  # The name of the image to pull
  name: splunk/fluentd-hec
  # The tag of the image to pull
  tag: 1.3.2
  # The policy that specifies when the user wants the images to be pulled
  pullPolicy: IfNotPresent
  # Indicates if the image should be pulled using authentication from a secret
  usePullSecret: false
  # The name of the pull secret to attach to the respective serviceaccount used to pull the image
  pullSecretName:

# Environment variable for daemonset
environmentVar:

# Pod annotations for daemonset
podAnnotations:

# Extra labels for daemonset, pods
extraLabels:

# Controls the resources used by the fluentd daemonset
resources:
  # limits:
  #  cpu: 100m
  #  memory: 200Mi
  requests:
    cpu: 100m
    memory: 200Mi

  # Controls the output buffer for the fluentd daemonset
  # Note that, for memory buffer, if `resources.limits.memory` is set,
  # the total buffer size should not bigger than the memory limit, it should also
  # consider the basic memory usage by fluentd itself.
  # All buffer parameters (except Argument) defined in
  # https://docs.fluentd.org/v1.0/articles/buffer-section#parameters
  # can be configured here.
bufferChunkKeys:
  - index
buffer:
  "@type": memory
  total_limit_size: 600m
  chunk_limit_size: 20m
  chunk_limit_records: 100000
  flush_interval: 5s
  flush_thread_count: 1
  overflow_action: block
  retry_max_times: 10
  retry_type: periodic
  retry_wait: 30

# set to true to keep the structure created by docker or journald
sendAllMetadata: false

# This default tolerations allow the daemonset to be deployed on master nodes,
# so that we can also collect logs from those nodes.
tolerations:
  - key: node-role.kubernetes.io/master
    effect: NoSchedule

# Defines which nodes should be selected to deploy the fluentd daemonset.
nodeSelector:
  kubernetes.io/os: linux

# Defines node affinity to restrict pod deployment.
affinity: {}

# Extra volumes required by pod
extraVolumes: []
extraVolumeMounts: []

# Defines priorityClassName to assign a priority class to pods.
priorityClassName:

# = Kubernetes Connection Configs =
kubernetes:
  # The cluster name used to tag logs. Default is cluster_name
  clusterName:
  # This flag specifies if the user wants to use a security context for creating the pods, which will be used to run privileged pods
  securityContext: false

# List of key/value pairs for metadata purpse.
# Can be used to define things such as cloud_account_id, cloud_account_region, etc.
customMetadata:
#  - name: "cloud_account_id"
#    value: "1234567890"

# List of annotation metadata you would like to enrich log data with
customMetadataAnnotations:
#  - name: custom_field
#    annotaion: splunk.com/custom_field

# `customFilters` defines the custom filters to be used.
# This section can be used to define custom filters using plugins like https://github.com/splunk/fluent-plugin-jq
# Its also possible to use other filters like https://www.fluentd.org/plugins#filter
#
# The scheme to define a custom filter is:
#
# ```
# <name>:
#   tag: <fluentd tag for the filter>
#   type: <fluentd filter type>
#   body: <definition of the fluentd filter>
# ```
#
# = fluentd tag for the filter =
# This is the fluentd tag for the record
#
# = fluentd filter type =
# This is the fluentd filter that the user wants to use for record manipulation.
#
# = definition of the fluentd filter =
# This defines the body/logic for using the filter for record manipulation.
#
# For example if you want to define a filter which sets cluster_name field to "my_awesome_cluster" you would the following filter
# <filter tail.containers.**>
#  @type jq_transformer
#  jq '.record.cluster_name = "my_awesome_cluster" | .record'
# </filter>
# This can be defined in the customFilters section as follows:
# ```
# customFilters:
#   NamespaceSourcetypeFilter:
#     tag: tail.containers.**
#     type: jq_transformer
#     body: jq '.record.cluster_name = "my_awesome_cluster" | .record'
# ```
customFilters: {}

#
# You can find more information on indexed fields here - http://dev.splunk.com/view/event-collector/SP-CAAAFB6
# The scheme to define an indexed field is:
#
# ```
# ["field_1", "field_2"]
# ```
#
# `indexFields` defines the fields from the fluentd record to be indexed.
# You can find more information on indexed fields here - http://dev.splunk.com/view/event-collector/SP-CAAAFB6
# The input is in the form of an array(comma separated list) of the values you want to use as indexed fields.
#
# For example if you want to define indexed fields for "field_1" and "field_2"
# you will have to define an indexFields section as follows in values.yaml file.
# ```
# indexFields: ["field_1", "field_2"]
# ```
# WARNING: The fields being used here must be available inside the fluentd record.
indexFields: []

# Configure DaemonSet .spec.updateStrategy.rollingUpdate params if you want to speed up
# rolling out a new version. By default only one pod after another is replaced when
# upgrading.
#
# Example: replace 5 pods in parallel
#
# ```
# rollingUpdate:
#   maxUnavailable: 5
# ```
#
# See https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/
# or `kubectl explain daemonset.spec.updateStrategy.rollingUpdate` for
# documentation of available keys.
rollingUpdate:

# Global configurations
# These configurations will be used if the corresponding local configurations are not set.
# For example, if `global.logLevel` is set and `logLevel` is not set, `global.logLevel` will be used; if `logLevel` is set, it will be used regardless `global.logLevel` is set or not.
global:
  logLevel: info
  # If local splunk configurations are not present, the global ones will be used (if available)
  splunk:
    # It has exactly the same configs as splunk.hec does
    hec:
      host:
      port: 8088
      token:
      protocol: https
      indexName:
      insecureSSL: false
      clientCert:
      clientKey:
      caFile:
      fullUrl:
      consume_chunk_on_4xx_errors:
      gzip_compression: false
  kubernetes:
    clusterName: "cluster_name"
  prometheus_enabled: true
  monitoring_agent_enabled: true
  monitoring_agent_index_name:
  monitoring_agent_bind_address:
  metrics:
    service:
      enabled: true
      headless: true
  # deploy a ServiceMonitor object for usage of the PrometheusOperator
  serviceMonitor:
    enabled: false

    metricsPort: 24231
    interval: ""
    scrapeTimeout: "10s"

    additionalLabels: {}
